# -*- coding: utf-8 -*-
"""Swins_Transformer_Reimplementation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yHDP_g8uau9nSz87jyTuB_WbxAIjaPw_
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.checkpoint as checkpoint
import numpy as np
from timm.models.layers import DropPath, to_2tuple, trunc_normal_

from mmcv_custom import load_checkpoint
from mmdet.utils import get_root_logger
from ..builder import BACKBONES
import math

def window_partition(x, window_size):
  """
  args: 
      x: a batch of images, a tensor with dim = (batch size, height, width, channel)
      window_size: take int, the number of patches per window
  return: 
      windons: a batch of windows, 
               a tensor with dim = (total number of windows, number of patches per window, number of patches per window, channel)
  """
  batch_size, height, width, C = x.shape
  num_windows_h =  height // window_size
  num_windows_w =  width // window_size
  x= x.view(batch_size,num_windows_h, window_size, num_windows_w,window_size,C)
  windows = x.transpose(3,2).contiguous().view(-1, window_size, window_size, C)
  return windows

def window_reverse(windows, window_size, H, W):
  """
  this function reverses the window partition operation, combining windows into original images
  args:
      windows(tensor): dim = (number of windows per image * batch size, number of patches per window,number of patches per window, C)
      window_size(int): dim = number of patches per window
      H(int): number of patches in height
      W(int): number of patches in width

  returns:
      A tensor: dim= (batch size, heigh, width, channel)
  """

  patches_img = H*W # total number of patches in one image
  num_windows = patches_img / (window_size*window_size) # number of windows per image
  batch_size = int(windows.shape[0] / num_windows)
  num_windows_h =  H // window_size
  num_windows_w =  W // window_size
  x = windows.view(batch_size, num_windows_h, num_windows_w,window_size,window_size,-1)
  x = x.transpose(3,2).contiguous().view(batch_size, H, W, -1)
  return x

class Mlp(nn.Module):

    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        if (hidden_features is None) or hidden_features<=0:
            hidden_features=in_features
        if (out_features is None) or out_features<=0:
            out_features=in_features
        self.fullyc1 = nn.Linear(in_features, hidden_features)
        self.fullyc2 = nn.Linear(hidden_features, out_features)
        self.hidden=nn.GELU()
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        x = self.fullyc1(x)
        x = self.hidden(x)
        x = self.drop(x)
        x = self.fullyc2(x)
        x = self.drop(x)
        return x

class PatchEmbed(nn.Module):
  """
  this function takes an image as input and partitions the image into patches. 
  args:
      img_size(int): size of image. default value: 224
      patch_size(int): number of raw pixels in one patch. default value: 4
      in_chans(int): input channels. default value: 3 (RGB)
      embed_dim(int): the dimension of features per patch after linear project. default value: 96 
      norm_layer (nn.Module): Batch Normalization or Layer Normalization. Default: None

  return: 
      tensor: dim = (batch size, number of patches, dimension of embed)

  """
  def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):
        super().__init__()
        self.in_chans = in_chans
        self.embed_dim = embed_dim
        self.img_size = (img_size, img_size) 
        self.patch_size = (patch_size, patch_size) 
        patches_resolution = [img_size[0]//patch_size[0], img_size[1]//patch_size[1]]
        self.patches_resolution = patches_resolution
        self.num_patches = (img_size//patch_size) * (img_size//patch_size)
        self.linear_proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

        if norm_layer is not None:
            self.norm = norm_layer(embed_dim)
        else:
            self.norm = None

        def forward(self,x):
            batch_size, chans, height, width = x.shape
            assert height == self.img_size[0] and width == self.img_size[1], "Input image size doesn't match model."
            x = self.linear_proj(x)
            # Conv2d output ( batch_size, number of filter, heigh of feature map, width of feature map)
            # each position in a feature map represents a feature of a patch
            x = x.view(batch_size, self.embed_dim, self.num_patches).permute(0, 2, 1)
            if self.norm is not None:
               x = self.norm(x)
            return x

class PatchMerging(nn.Module):
  """
  this function takes a batch of windows as input 
                reduces window_size (the number of patches in each window in the height and in the width) by 1/2 
                increase the number of embedding(embed_dim) for each patch by 2
                first, slices each window into 4 slices, each slice takes step 2
                second, stacks 4 slices together in the emdedding dimension 
                next, output dim = (batch_size, window_size/2, window_size/2, 4* embed_dim)
                thrid, reshapes the dimensions into (batch_size, number of patches per windows /4, 4 * embed_dim )
                finally, project 4*embed_dim into 2_embed_dim

                all the patches in one windows collape into one patch and will be combined with other patches as 
                one window to do windows attention, in this way, windows attention operates on not only the local windows
                but also the surrounding windows. 
                Similar effect as the network gets deeper, the larger the kernel size of the deeper convolution layers is set.

  args:
      input_resolution( tuple(int, int)): windows size, (number of patches in height, number of patches in width)
      dim(int): dimension of embedding for each patch
      norm_layer(nn.Module, optional): batch normalization layer or layer normalization layer. default value: nn.LayerNorm 

  """
  def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):
      super().__init__()
      self.input_resolution = input_resolution
      self.dim = dim
      self.linear_proj = nn.Linear(4 * dim, 2 * dim, bias=False)
      self.norm = norm_layer(4 * dim)
  def forward(self, x):
    """
    x: a tensor of a batch of windows, dim=( batch_size, number of patches per windows( windon_size * windon_size), embed_dim)

    """
    H, W = self.input_resolution
    batch_size, number_patches, embed_dim = x.shape
    assert number_patches == H * W, "wrong size"
    assert H % 2 == 0 and W % 2 == 0, f"x size ({H}*{W}) are not even."
    x = x.view(batch_size, H, W, embed_dim) # assume H==W==4
    x0 = x[:, 0::2, 0::2, :]  # [batch_size, [0, 2], [0, 2], embed_dim]
    x1 = x[:, 1::2, 0::2, :]  # [batch_size, [1, 3], [0, 2], embed_dim]
    x2 = x[:, 0::2, 1::2, :]  # [batch_size, [0, 2], [1, 3], embed_dim]
    x3 = x[:, 1::2, 1::2, :]  # [batch_size, [1, 3], [1, 3], embed_dim]
    x = torch.cat([x0, x1, x2, x3], -1)  # concatenate all the slices in the embed dimension
    x = x.view(batch_size, -1, 4 * embed_dim) #  # B H/2 W/2 4*C
    x = self.norm(x)
    x = self.linear_proj(x)
    return x
  def extra_repr(self) -> str:
    return f"input_resolution={self.input_resolution}, dim={self.dim}"

class WindowAttention(nn.Module):

    def __init__(self, channels_number, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):

        super().__init__()
        self.cn = channels_number
        self.window_h=window_size[0]
        self.window_w=window_size[1]
        self.num_heads = num_heads
        self.qkv_bias=qkv_bias
        self.qk_scale = qk_scale or 1/math.sqrt(channels_number// num_heads)
        self.position_bias = nn.Parameter(torch.zeros((2 * self.window_h - 1) * (2 * self.window_w - 1), self.num_heads))
        self.qkv = nn.Linear(self.cn, 3*self.cn, bias=self.qkv_bias)
        self.attn_drop = nn.Dropout(attn_drop)
        self.proj = nn.Linear(self.cn, self.cn)
        self.proj_drop = nn.Dropout(proj_drop)
        trunc_normal_(self.position_bias, std=0.02)
        self.softmax = nn.Softmax(dim=-1)

        #change dimension to be (2, h*w)
        coords = torch.flatten(torch.stack(torch.meshgrid([torch.arange(0,self.window_h), torch.arange(0,self.window_w)])),1)
        
        new_coords = (coords[:, :, None] - coords[:, None, :]).permute(1, 2, 0).contiguous()
        for i in range(2):
            new_coords[:, :, i] = new_coords[:, :, i]+window_size[i] - 1 

        new_coords[:, :, 0] = 2*new_coords[:, :, 0]* self.window_w - 1
        #change dimension to be (hw,hw)
        new_position = new_coords.sum(-1)  
        self.register_buffer("relative_position_index", new_position)

    def forward(self, x, mask=None):
        
        new_position_bias = self.position_bias[self.new_position.view(-1)]
        new_position_bias = new_position_bias.view(self.window_h * self.window_w, self.window_h * self.window_w, -1).permute(2, 0, 1).contiguous()  # Wh*Ww,Wh*Ww,nH

        org_qkv = self.qkv(x).reshape(x.shape[0], x.shape[1], 3, self.num_heads, x.shape[2] // self.num_heads)
        new_qkv=org_qkv.permute(2, 0, 3, 1, 4)

        new_qkv[0] = self.qk_scale*new_qkv[0]

        attention = torch.dot(new_qkv[0],new_qkv[1].transpose(-2, -1)) + new_position_bias.unsqueeze(0)
        ax=x.shape[0] // mask.shape[0]
        
        if mask is not None:
            attention = attention.view(ax, mask.shape[0], self.num_heads, x.shape[1], x.shape[1])
            attention = attention.view(-1, self.num_heads, x.shape[1], x.shape[1])
            attention = self.softmax(attention+mask.unsqueeze(1).unsqueeze(0))
            attention = self.attn_drop(attention)

        else:
            attention = self.attn_drop(self.softmax(attention))

    

        x = torch.dot(attention, v).transpose(1, 2)
        x = x.reshape(x.shape[0], x.shape[1], x.shape[2])
        x = self.proj_drop(self.proj(x))
        return x

class SwinTransformerBlock(nn.Module):
    r""" Swin Transformer Block.

    step 1: layer norm
    step 2: window/ shifted window self attension 
    step 3: skip_connection + layer norm 
    step 4: MLP 

  
    Args:
        dim (int): Number of input channels. # embediing dimension
        input_resolution (int, int): Input resulotion. # number of patches in height or width
        num_heads (int): Number of attention heads. # number of different patterns
        window_size (int): Window size. 
                        # number of patches in heigh or width per window (in window partition). default: 7, each window is made of 7*7 patches
        shift_size (int): Shift size for SW-MSA. # number of patches shifted in the window partition
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.
        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True
        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set. # normalized factor, when dimension of keys is larger
        drop (float, optional): Dropout rate. Default: 0.0  # regulization 
        attn_drop (float, optional): Attention dropout rate. Default: 0.0
        drop_path (float, optional): Stochastic depth rate. Default: 0.0
        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU
        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm
    """

    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,
                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,
                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.dim = dim
        self.input_resolution = input_resolution # output from patch embedding, number of patches in height or width.
        self.num_heads = num_heads
        self.window_size = window_size  # number of patches per windonw in height or width used in window partition.
        self.shift_size = shift_size
        self.mlp_ratio = mlp_ratio

        if min(self.input_resolution) <= self.window_size: # 
            # if input size is less than the required size, then whole image is regarded as one window 
            self.shift_size = 0 # no surrounding windows, so skip the shift window operation
            self.window_size = min(self.input_resolution) # truncate the image in order to keep the window_size same in height and width
        assert 0 <= self.shift_size < self.window_size, "can't shift over the window size" 
        
        #step 1: layer norm
        self.norm1 = norm_layer(dim) 

        # step 2: window/ shifted window self attension
        self.attn = WindowAttention(
            dim, window_size=(self.window_size,self.window_size), num_heads=num_heads,
            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        # step 3: layer norm
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()
        self.norm2 = norm_layer(dim)
        
        #step 4: MLP
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

        if self.shift_size > 0:
            # calculate attention mask for SW-MSA
            H, W = self.input_resolution
            img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1
            h_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            w_slices = (slice(0, -self.window_size),
                        slice(-self.window_size, -self.shift_size),
                        slice(-self.shift_size, None))
            cnt = 0
            for h in h_slices:
                for w in w_slices:
                    img_mask[:, h, w, :] = cnt
                    cnt += 1
            # windows_partition returns (number of windows per image * batch size, window_size, window_size[number of patches in height/width], embed_dim )
            mask_windows = window_partition(img_mask, self.window_size)  # mask_windows = (number of windows per image, window_size,window_size, 1)
            mask_windows = mask_windows.view(-1, self.window_size * self.window_size)   ## reshape = (number of windows, number of patches per windows) 
            attn_mkx= mask_windows.unsqueeze(1) # return (number of windows, 1, number of patches per window)
            attn_mky= mask_windows.unsqueeze(2) # return (number of windows,  number of patches per window, 1)
            #  boardcast to the same shape
            # 4 different types of mask
            attn_mask = attn_mkx - attn_mky # return (number of windows,number of patches per window, number of patches per window )
            # if attention is not from the surrounding area, indicated by nonzero value, fills with negative value
            attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)) 
            # if attention is actually from the surrounding area, indicated by zero value, fills with 0.0
            attn_mask = attn_mask.masked_fill(attn_mask == 0, float(0.0))
        else:
            attn_mask = None

        self.register_buffer("attn_mask", attn_mask)

    def forward(self, x):
        H, W = self.input_resolution # window_size
        B, num_patches, C = x.shape
        assert num_patches == H * W, "input feature has wrong size"

        shortcut = x 
        #step 1: layer norm
        x = self.norm1(x)

        x = x.view(B, H, W, C) 

        # shift windows or local windows
        # first move all rows up shift_size units, first shift_size rows move down to the bottom
        # next move all columns left to shift_size units, first shift_size columns move down to the right
        if self.shift_size > 0: 
            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))  
        else:
            shifted_x = x

        # partition windows, return (number of windows* batch size, number of patches per windon,number of patches per windon, embed_dim)
        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C
        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # (total number of windows, total number of patches, embed_dim)

        # local window attension or shifted window attention, return  same shape of input 
        # embedding for each patch is weighted by the attention score.
        # patches per windows as key and quary, embedding per patch as value
        attn_windows = self.attn(x_windows, mask=self.attn_mask)  # (total number of windows, total number of patches, embed_dim)

        # reserves window partition
        # window_reverse: args: (a batch of windows, number of patches per window, number of patches in height per image, number of patches in width per image)
        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)
        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # return (batch size, number of patches in height per image,number of patches in width per image, embed_dim)

        # reverse cyclic shift
        # first move all rows down shift_size units, last shift_size rows move to the top 
        # next move all columns right to shift_size units, last shift_size columns move to the left
        if self.shift_size > 0: 
            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))
        else:
            x = shifted_x
        x = x.view(B, H * W, C)

        # shortcut is the original input, x is weighted attention scored output
        res = shortcut + self.drop_path(x) # skip connection between input and output of W-MSA
        output = res + self.drop_path(self.mlp(self.norm2(res)))

        return output

class BasicLayer(nn.Module):

    def __init__(self,dim,depth,num_heads,window_size=7,mlp_ratio=4.,qkv_bias=True,qk_scale=None,drop=0.,attn_drop=0.,drop_path=0.,norm_layer=nn.LayerNorm,downsample=None,use_checkpoint=False):
        super().__init__()
        self.cn=dim
        self.depth = depth
        self.num_heads=num_heads
        self.window_size = window_size
        self.mlp_ratio=mlp_ratio
        self.use_checkpoint = use_checkpoint
        if downsample is not None:
            self.downsample = downsample(dim=dim, norm_layer=norm_layer)
        else:
            self.downsample = None
        self.blocks = nn.ModuleList([SwinTransformerBlock(dim=dim,num_heads=num_heads,window_size=window_size,shift_size=0 if (i % 2 == 0) else window_size // 2,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,drop=drop,attn_drop=attn_drop,drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,norm_layer=norm_layer) for i in range(depth)])

    def forward(self, x, H, W):

        # SW-MSA
        flag = 0
        sliceh = (slice(0, -self.window_size),slice(-self.window_size, -self.window_size // 2),slice(-self.window_size // 2, None))
        slicew = (slice(0, -self.window_size),slice(-self.window_size, -self.window_size // 2),slice(-self.window_size // 2, None))
        mask = torch.zeros((1, self.window_size*int(np.ceil(H / self.window_size)), self.window_size*int(np.ceil(W / self.window_size)), 1), device=x.device)
        
        for i in sliceh:
            for j in slicew:
                mask[:, i, j, :] = flag
                flag =flag+ 1

        mw = window_partition(mask, self.window_size).view(-1, self.window_size * self.window_size)
        ma = mw.unsqueeze(1) - mw.unsqueeze(2)
        new_mask = ma.masked_fill(ma!= 0, float(-100.0))
        att_mask = new_mask.masked_fill(ma == 0, float(0.0))

        for b in self.blocks:
            b.H= H
            b.W=W
            if self.use_checkpoint:
                x = checkpoint.checkpoint(b, x, att_mask)
            else:
                x = b(x, att_mask)
        
        if self.downsample is not None:
            return x, H, W, self.downsample(x, H, W), (H + 1) // 2, (W + 1) // 2
        else:
            return x, H, W, x, H, W

# def window_partition1(x, window_size):
#     """
#     Args:
#         x: (B, H, W, C)
#         window_size (int): window size
#     Returns:
#         windows: (num_windows*B, window_size, window_size, C)
#     """
#     B, H, W, C = x.shape
#     x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)
#     windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
#     return windows

# def window_reverse1(windows, window_size, H, W):
#     """
#     Args:
#         windows: (num_windows*B, window_size, window_size, C)
#         window_size (int): Window size
#         H (int): Height of image
#         W (int): Width of image
#     Returns:
#         x: (B, H, W, C)
#     """
#     B = int(windows.shape[0] / (H * W / window_size / window_size))
#     x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)
#     x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)
#     return x

# # test code
# x = torch.randn(5, 4,4, 3)
# print("input",x.shape)
# orig = window_partition1(x, 2)
# print("orig",orig.shape)
# new = window_partition1(x, 2)
# print("new",new.shape)
# orig_re = window_reverse1(orig, 2, 4, 4)
# print("orig_re",orig_re.shape)
# new_re = window_reverse(orig, 2, 4, 4)
# print("new_re",new_re.shape)

@BACKBONES.register_module()
class SwinTransformer(nn.Module):

    def __init__(self,pretrain_img_size=224,patch_size=4,in_chans=3,embed_dim=96,depths=[2, 2, 6, 2],num_heads=[3, 6, 12, 24],window_size=7,mlp_ratio=4.,qkv_bias=True,qk_scale=None,drop_rate=0.,attn_drop_rate=0.,drop_path_rate=0.2,norm_layer=nn.LayerNorm,ape=False,patch_norm=True,out_indices=(0, 1, 2, 3),frozen_stages=-1,use_checkpoint=False):
        super().__init__()

        self.pretrain_img_size = pretrain_img_size
        self.patch_size = patch_size
        self.nchan=in_chans
        self.embed_dim = embed_dim
        self.depths=depths
        self.num_features =[]
        for d in range(len(self.depths)):
            self.num_features.append(int(math.pow(2,i)*embed_dim))
        self.ape = ape
        self.patch_norm = patch_norm
        self.out_indices = out_indices
        for i in out_indices:
            self.add_module(f'norm{i}', norm_layer(num_features[i]))
        self.frozen_stages = frozen_stages
        self.patch_embed = PatchEmbed(patch_size=self.patch_size, in_chans=self.nchan, embed_dim=self.embed_dim,norm_layer=norm_layer if self.patch_norm else None)
        self.pos_drop = nn.Dropout(p=drop_rate)
        self.layers = nn.ModuleList()
        
        path=[]
        for l in torch.linspace(0, drop_path_rate, sum(self.depths)):
            path.append(l.item())

        if self.ape:
            rs = [to_2tuple(pretrain_img_size)[0] // to_2tuple(patch_size)[0], to_2tuple(pretrain_img_size)[1] // to_2tuple(patch_size)[1]]
            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, self.embed_dim, rs[0], rs[1]))
            trunc_normal_(self.absolute_pos_embed, std=0.02)


        for depth in range(len(self.depths)):
            layer = BasicLayer(dim=int(math.pow(2,depth)*embed_dim),depth=depths[depth],num_heads=num_heads[depth],window_size=window_size,mlp_ratio=mlp_ratio,qkv_bias=qkv_bias,qk_scale=qk_scale,drop=drop_rate,attn_drop=attn_drop_rate,drop_path=path[sum(depths[:depth]):sum(depths[:depth + 1])],norm_layer=norm_layer,downsample=PatchMerging if (depth < len(self.depths) - 1) else None,use_checkpoint=use_checkpoint)
            self.layers.append(layer)


        self._freeze_stages()

    def init_weights(self, pretrained=None):
        
        def _init_weights(m):
            if isinstance(m, nn.Linear):
                trunc_normal_(m.weight, std=.02)
                if isinstance(m, nn.Linear) and m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.bias, 0)
                nn.init.constant_(m.weight, 1.0)

        if isinstance(pretrained, str):
            self.apply(_init_weights)
            logger = get_root_logger()
            load_checkpoint(self, pretrained, strict=False, logger=logger)
        elif pretrained is None:
            self.apply(_init_weights)
        else:
            raise TypeError('pretrained must be a str or None')

    def _freeze_stages(self):
        if self.frozen_stages >= 0:
            self.patch_embed.eval()
            for p in self.patch_embed.parameters():
                param.requires_grad = False

        if self.frozen_stages >= 1 and self.ape:
            self.absolute_pos_embed.requires_grad = False

        if self.frozen_stages >= 2:
            self.pos_drop.eval()
            for i in range(self.frozen_stages - 1):
                m=self.layers[i]
                m.eval()
                for param in m.parameters():
                    param.requires_grad = False

    def forward(self, x):
        output = []
        x = self.patch_embed(x)

        if self.ape:
            x = (x + F.interpolate(self.absolute_pos_embed, size=(x.size(2), x.size(3)), mode='bicubic'))

        x = x.flatten(2).transpose(1, 2)
        x = self.pos_drop(x)

        for i in range(len(self.depths)):
            layer = self.layers[i]
            out1, H, W, x, _, _ = layer(x, x.size(2), x.size(3))

            if i in self.out_indices:
                norm_layer = getattr(self, f'norm{i}')
                output.append(norm_layer(out1).view(-1, H, W, self.num_features[i]).permute(0, 3, 1, 2).contiguous())

        return tuple(output)

    def train(self, mode=True):
        super(SwinTransformer, self).train(mode)
        self._freeze_stages()